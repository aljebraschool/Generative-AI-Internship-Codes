{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNpMU9M5lGjtwY5EJ82Y6O2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aljebraschool/Generative-AI-Internship-Codes/blob/main/1_prompt_engineering_overview_part1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Overview of Prompt Engineering Techniques & Best Practices"
      ],
      "metadata": {
        "id": "giGy-Wa6vqD-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Prompt Engineering Best Practices\n",
        "\n",
        "In this section, we provide an overview of the top tips and best practices for prompting LLMs."
      ],
      "metadata": {
        "id": "5zETiQAjvrFv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We first load the necessary libraries:"
      ],
      "metadata": {
        "id": "lG-WRbnDv2Op"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3hQxPG7uQVN",
        "outputId": "77a77052-4fc3-41a0-c97f-747aaf81fc96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/76.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/76.5 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m71.7/76.5 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m718.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "! pip install openai==0.28 --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai #open ai library\n",
        "import os     #managing directory/folder\n",
        "import IPython #getting formatted printout"
      ],
      "metadata": {
        "id": "bNlcKC9lv5ju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#open API key configuration\n",
        "openai.api_key = \"OPENAI_API_KEY\""
      ],
      "metadata": {
        "id": "JdyqLMjJwEFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#This is a function that perform completion task for a text input given to it\n",
        "#here we set the model, temperate (variation in output), max_token (longest output/input)\n",
        "def get_completion(message, model= 'gpt-3.5-turbo', temperature = 0, max_tokens = 300):\n",
        "  response = openai.ChatCompletion.create(\n",
        "      model = model,\n",
        "      messages = message,\n",
        "      temperature = temperature,\n",
        "      max_tokens = max_tokens,\n",
        "\n",
        "  )\n",
        "  return response.choices[0].message"
      ],
      "metadata": {
        "id": "tUCbiib3wgni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Be Specific and Clear"
      ],
      "metadata": {
        "id": "hJ11-m26yK6x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write instructions as clear and specific as possible to get the desired LLM behaviors:"
      ],
      "metadata": {
        "id": "3aQ83dqByQKA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "global_trending_movies = [\"The Suicide Squad\", \"No Time to Die\", \"Dune\",  \"Spider-Man: No Way Home\", \"The French Dispatch\", \"Black Widow\", \"Eternals\", \"The Matrix Resurrections\", \"West Side Story\", \"The Many Saints of Newark\"]\n",
        "\n",
        "system_message = \"\"\"\n",
        "\n",
        "Your task is to recommend movies to a customer.\n",
        "\n",
        "You are to recommend movie from the the list of {global_trending_movies}\n",
        "\n",
        "You should refrain from asking user for preference any other personal information.\n",
        "\n",
        "If you don't have a movie to recommend or don't know the user interest, you should respond by saying \"Sorry, couldn't find a movie to recommend taday! \"\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "user_request = \"\"\"\n",
        "Please recommend a movie based on my interests.\n",
        "\"\"\"\n",
        "\n",
        "message = [\n",
        "\n",
        "    {'role': 'system',\n",
        "     'content': system_message.format(global_trending_movies = global_trending_movies)},\n",
        "\n",
        "    {'role': 'user',\n",
        "     'content': user_request}\n",
        "\n",
        "]\n",
        "\n",
        "#now call the get_completion function and pass in the massage\n",
        "\n",
        "response = get_completion(message)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAeVj2nUxj1M",
        "outputId": "a8f7bb7a-6409-4cb7-a361-b35ad3e8b0f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"role\": \"assistant\",\n",
            "  \"content\": \"Sorry, couldn't find a movie to recommend today!\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The more specific behavior you want from the model, the more specific the instructions and logic should be. Below is an example where the customer provides information about interests:"
      ],
      "metadata": {
        "id": "c_dc1lvG6Irj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "global_trending_movies = [\"The Suicide Squad\", \"No Time to Die\", \"Dune\",  \"Spider-Man: No Way Home\", \"The French Dispatch\", \"Black Widow\", \"Eternals\", \"The Matrix Resurrections\", \"West Side Story\", \"The Many Saints of Newark\"]\n",
        "\n",
        "system_message = \"\"\"\n",
        "Your task is to recommends movies to a customer.\n",
        "\n",
        "You are responsible to recommend a movie from the top global trending movies from {global_trending_movies}.\n",
        "\n",
        "You should refrain from asking users for their preferences and avoid asking for personal information.\n",
        "\n",
        "If you don't have a movie to recommend or don't know the user interests, you should respond \"Sorry, couldn't find a movie to recommend today.\".\n",
        "\"\"\"\n",
        "\n",
        "user_request = \"\"\"\n",
        "I love super-hero movies. Please recommend a movie based using the global_trending_movies.\n",
        "\"\"\"\n",
        "\n",
        "message = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": system_message.format(global_trending_movies=global_trending_movies)\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": user_request\n",
        "    }\n",
        "]\n",
        "\n",
        "response = get_completion(message)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1i1DjGI5Gqy",
        "outputId": "ebbeb931-2dad-45e6-bed4-ba910df65dcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"role\": \"assistant\",\n",
            "  \"content\": \"I recommend you watch \\\"Spider-Man: No Way Home\\\" from the list of global trending movies. It's a great superhero movie that I think you'll enjoy!\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add Delimiters\n",
        "\n",
        "Adding delimiters help to better structure instructions and the overall prompt components. This is beneficial to get more reliable responses."
      ],
      "metadata": {
        "id": "_1FW4xer68Ez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "convert the following code block in the ### <code> ### to python\n",
        "####\n",
        "strings2.push(\"one\")\n",
        "strings2.push(\"two\")\n",
        "strings2.push(\"THREE\")\n",
        "strings2.push(\"4\")\n",
        "####\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "message = [\n",
        "    {\n",
        "        'role':'user',\n",
        "        'content':prompt\n",
        "    }\n",
        "]\n",
        "\n",
        "response = get_completion(message)\n",
        "\n",
        "IPython.display.Markdown(response['content'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "y9Uo5gU35oZf",
        "outputId": "81e8f0e8-35d0-466f-9776-0a319e0404c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "```python\nstrings2.append(\"one\")\nstrings2.append(\"two\")\nstrings2.append(\"THREE\")\nstrings2.append(\"4\")\n```"
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Specify Output Format\n",
        "\n",
        "If the format of prompt responses are important, then this should be explicitly stated in the prompt to get desired results. In the example, we would like to export the results as a JSON object."
      ],
      "metadata": {
        "id": "fnbDYwq59MC3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "Your task is: given a product description, return the requested information in the section delimited by ### ###. Format the output as a JSON object.\n",
        "\n",
        "Product Description: Introducing the Nike Air Max 270 React: a comfortable and stylish sneaker that combines two of Nike's best technologies.\n",
        "With a sleek black design and a unique bubble sole, these shoes are perfect for everyday wear.\n",
        "\n",
        "###\n",
        "product_name: the name of the product\n",
        "product_bran: the name of the brand (if any)\n",
        "###\n",
        "\"\"\"\n",
        "\n",
        "message = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt\n",
        "    }\n",
        "]\n",
        "\n",
        "response = get_completion(message)\n",
        "print(response['content'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f682xUc18zAs",
        "outputId": "3c4567cb-e1e1-4bf3-fd09-05dc2e184d69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"product_name\": \"Nike Air Max 270 React\",\n",
            "    \"product_brand\": \"Nike\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Think Step by Step\n",
        "\n",
        "For explicit reasoning in LLMs, you can prompt the model to think step-by-step. Prompting the model in this way allows it to provide the details steps before providing a final response that solves the problem."
      ],
      "metadata": {
        "id": "8Hs6kFFm_Jj4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\" The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.\n",
        "\n",
        "Solve this problem by braking it down in steps of reasoning.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "message = [\n",
        "    {\n",
        "        'role': 'user',\n",
        "        'content': prompt\n",
        "    }\n",
        "]\n",
        "\n",
        "response = get_completion(message)\n",
        "print(response['content'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWU5YjIJ-QUR",
        "outputId": "34f1faaa-910a-4358-e300-98b83d799dff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1: Identify the odd numbers in the group: 15, 5, 13, 7, 1.\n",
            "\n",
            "Step 2: Add up the odd numbers: 15 + 5 + 13 + 7 + 1 = 41.\n",
            "\n",
            "Step 3: Determine if the sum of the odd numbers is even or odd. In this case, 41 is an odd number.\n",
            "\n",
            "Step 4: Since the sum of the odd numbers is odd, the statement that the odd numbers in the group add up to an even number is false.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Role Playing\n",
        "\n",
        "The example below shows how to apply role playing using a chat model like GPT-3.5 Turbo. Notice the use of system message, user message, and assistant message in the example. You can combine different messages to mimic or jump start the behavior you want or expect from the model."
      ],
      "metadata": {
        "id": "5_GYO-t0ALEC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_message = \"\"\"\n",
        "The following is a conversation with an AI research assistant. The assistant tone is technical and scientific.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "user_message_1 = \"\"\"\n",
        "Hello, who are you?\n",
        "\"\"\"\n",
        "\n",
        "ai_message_1 = \"\"\"\n",
        "Greeting! I am an AI research assistant. How can I help you today?\n",
        "\"\"\"\n",
        "\n",
        "prompt = \"\"\"\n",
        "Human: Can you tell me about the creation of blackholes?\n",
        "AI:\n",
        "\"\"\"\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": system_message\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": user_message_1\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"assistant\",\n",
        "        \"content\": ai_message_1\n",
        "\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt\n",
        "    }\n",
        "]\n",
        "\n",
        "response = get_completion(messages)\n",
        "IPython.display.Markdown(response['content'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "JfHeAeet_2f6",
        "outputId": "2cafda13-c233-4da3-d642-c0aa2c35146e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Black holes are formed when massive stars undergo gravitational collapse at the end of their life cycle. This collapse occurs when the star's core runs out of nuclear fuel and can no longer support itself against the force of gravity. The core collapses under its own weight, leading to a singularity - a point of infinite density and zero volume. The gravitational pull of this singularity is so strong that not even light can escape, creating what we observe as a black hole."
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CcfKDuAHAYjs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}